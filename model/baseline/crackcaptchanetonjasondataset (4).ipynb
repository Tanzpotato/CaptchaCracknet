{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5183554,"sourceType":"datasetVersion","datasetId":3013698}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-13T07:26:32.627299Z","iopub.execute_input":"2025-05-13T07:26:32.627532Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# link to the Dataset -> https://www.kaggle.com/code/egregiouslytalented/crackcaptchanetonjasondataset?scriptVersionId=239239503&cellId=5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:30:55.613514Z","iopub.execute_input":"2025-05-12T16:30:55.613799Z","iopub.status.idle":"2025-05-12T16:30:55.617379Z","shell.execute_reply.started":"2025-05-12T16:30:55.613776Z","shell.execute_reply":"2025-05-12T16:30:55.616667Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip install --upgrade torch-summary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:30:58.192918Z","iopub.execute_input":"2025-05-12T16:30:58.193383Z","iopub.status.idle":"2025-05-12T16:31:02.132002Z","shell.execute_reply.started":"2025-05-12T16:30:58.193345Z","shell.execute_reply":"2025-05-12T16:31:02.131229Z"}},"outputs":[{"name":"stdout","text":"Collecting torch-summary\n  Downloading torch_summary-1.4.5-py3-none-any.whl.metadata (18 kB)\nDownloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\nInstalling collected packages: torch-summary\nSuccessfully installed torch-summary-1.4.5\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torchsummary import summary\nfrom tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:31:05.653890Z","iopub.execute_input":"2025-05-12T16:31:05.654146Z","iopub.status.idle":"2025-05-12T16:31:07.247559Z","shell.execute_reply.started":"2025-05-12T16:31:05.654126Z","shell.execute_reply":"2025-05-12T16:31:07.247021Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"DATA_DIR='/kaggle/input/comprasnet-captchas/comprasnet_imagensacerto'\n#Model Hyperparamters\nBATCH_SIZE=128\nVAL_SPLIT=0.05\n#CRNN\nCRNN_KERNEL=5\nCRNN_POOL_KERNEL=2\nCRNN_DROPOUT=0.3\nCRNN_LATENT=128\nLSTM_HIDDEN_DIM=32\nVOCAB_SIZE=26*2+10\nOUTPUT_LENGTH=6\n#AFFN\nAFFN_KERNEL=5\nAFFN_STRIDE=1\nAFFN_DEPTH=4\n\n#CRNN\nCRNN_KERNEL=5\nCRNN_POOL_KERNEL=2\nCRNN_DROPOUT=0.3\nCRNN_LATENT=128\nLSTM_HIDDEN_DIM=32\nVOCAB_SIZE=26*2+10\nOUTPUT_LENGTH=6\n\n\nSAVE_EPOCH=10\nVAL_EPOCH=1\nEPOCHS=40","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:42:39.575171Z","iopub.execute_input":"2025-05-12T16:42:39.575475Z","iopub.status.idle":"2025-05-12T16:42:39.581097Z","shell.execute_reply.started":"2025-05-12T16:42:39.575450Z","shell.execute_reply":"2025-05-12T16:42:39.580275Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:34:52.204400Z","iopub.execute_input":"2025-05-12T16:34:52.204686Z","iopub.status.idle":"2025-05-12T16:34:52.208375Z","shell.execute_reply.started":"2025-05-12T16:34:52.204664Z","shell.execute_reply":"2025-05-12T16:34:52.207696Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import os\nimport torch\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, random_split, Dataset\nfrom PIL import Image\nimport string\n\n\n\ndef get_dataloaders(data_dir, batch_size, val_split, shuffle=True, num_workers=2):\n    \"\"\"\n    Creates PyTorch dataloaders for training and validation with one-hot encoded labels.\n    \n    Parameters:\n        data_dir (str): Path to the folder containing images.\n        batch_size (int): Batch size for dataloaders.\n        val_split (float): Fraction of data to use for validation.\n        shuffle (bool): Whether to shuffle data.\n        num_workers (int): Number of workers for dataloaders.\n\n    Returns:\n        train_loader, val_loader: DataLoaders for training and validation.\n    \"\"\"\n    # Define the character set (vocabulary)\n    characters = string.ascii_letters + string.digits  # Uppercase + lowercase + digits\n    char_to_idx = {char: idx for idx, char in enumerate(characters)}\n    vocab_size = len(characters)\n\n    class CustomDataset(Dataset):\n        def __init__(self, root_dir, transform=None):\n            self.root_dir = root_dir\n            self.transform = transform\n            self.image_paths = []\n            self.labels = []\n\n            for f in os.listdir(root_dir):\n                if f.endswith(('png', 'jpg', 'jpeg')):\n                    label = os.path.splitext(f)[0]\n                if len(label) == 6:\n                    self.image_paths.append(os.path.join(root_dir, f))\n                    self.labels.append(label)\n\n        def __len__(self):\n            return len(self.image_paths)\n        \n        def __getitem__(self, idx):\n            img_path = self.image_paths[idx]\n            image = Image.open(img_path).convert('RGB')\n            label_str = self.labels[idx] \n            \n            # Convert label string to one-hot encoded tensor\n            label_indices = [char_to_idx[c] for c in label_str if c in char_to_idx]  # Map characters to indices\n            label_tensor = torch.zeros(len(label_indices),dtype=torch.long)  # One-hot encoding tensor\n            for i, index in enumerate(label_indices):\n                label_tensor[i] = index  # Set one-hot encoding\n            \n            if self.transform:\n                image = self.transform(image)\n            \n            return image, label_tensor\n    \n    \n    transform = transforms.Compose([\n        transforms.Resize((40, 150)),  # Resize to a fixed size\n        transforms.ToTensor(),\n        transforms.Grayscale(),\n    ]) \n    dataset = CustomDataset(root_dir=data_dir, transform=transform)\n    \n    # Compute train-validation split\n    total_size = len(dataset)\n    val_size = int(total_size * val_split)\n    train_size = total_size - val_size\n    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers,)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers,)\n    \n    return train_loader, val_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:34:53.579530Z","iopub.execute_input":"2025-05-12T16:34:53.580195Z","iopub.status.idle":"2025-05-12T16:34:53.594032Z","shell.execute_reply.started":"2025-05-12T16:34:53.580171Z","shell.execute_reply":"2025-05-12T16:34:53.593122Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_loader,val_loader=get_dataloaders(DATA_DIR, batch_size=BATCH_SIZE, val_split=VAL_SPLIT, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:34:56.848846Z","iopub.execute_input":"2025-05-12T16:34:56.849393Z","iopub.status.idle":"2025-05-12T16:34:56.982108Z","shell.execute_reply.started":"2025-05-12T16:34:56.849370Z","shell.execute_reply":"2025-05-12T16:34:56.981525Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"for X,y in train_loader:\n    print(X.shape)\n    print(y.shape)\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:34:58.711417Z","iopub.execute_input":"2025-05-12T16:34:58.711990Z","iopub.status.idle":"2025-05-12T16:35:01.274412Z","shell.execute_reply.started":"2025-05-12T16:34:58.711965Z","shell.execute_reply":"2025-05-12T16:35:01.273715Z"}},"outputs":[{"name":"stdout","text":"torch.Size([128, 1, 40, 150])\ntorch.Size([128, 6])\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"class Encoder(nn.Sequential):\n    def __init__(self,n,kernel_size,stride):\n        super().__init__(\n            nn.Conv2d(in_channels=4**(n-1),out_channels=4**n,kernel_size=kernel_size,stride=stride),\n            nn.BatchNorm2d(num_features=4**n),\n            nn.ReLU(inplace=False)\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:35:03.769974Z","iopub.execute_input":"2025-05-12T16:35:03.770727Z","iopub.status.idle":"2025-05-12T16:35:03.776743Z","shell.execute_reply.started":"2025-05-12T16:35:03.770689Z","shell.execute_reply":"2025-05-12T16:35:03.775792Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class Decoder(nn.Sequential):\n    def __init__(self,n,kernel_size,stride):\n        super().__init__(\n            nn.ConvTranspose2d(in_channels=4**n,out_channels=4**(n-1),kernel_size=kernel_size,stride=stride),\n            nn.BatchNorm2d(num_features=4**(n-1)),\n            nn.ReLU(inplace=False)\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:35:06.334526Z","iopub.execute_input":"2025-05-12T16:35:06.335042Z","iopub.status.idle":"2025-05-12T16:35:06.339494Z","shell.execute_reply.started":"2025-05-12T16:35:06.335018Z","shell.execute_reply":"2025-05-12T16:35:06.338764Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class AFFN(nn.Module):\n    def __init__(self,n):\n        super().__init__()\n        self.n=n\n        # self.test=nn.Linear(1,2)\n        self.alpha=nn.Parameter(torch.randn(n-1).to(device)).to(device)\n        self.encoders=[]\n        for i in range(1,n+1):\n            self.encoders.append(Encoder(i,AFFN_KERNEL,AFFN_STRIDE).to(device))\n\n        self.decoders=[]\n        for i in range(n,0,-1):\n            self.decoders.append(Decoder(i,AFFN_KERNEL,AFFN_STRIDE).to(device))\n            \n    def forward(self, x):\n        residuals = []\n        for i, enc in enumerate(self.encoders):\n            x = enc(x)\n            if i < self.n - 1:\n                x = x * (1 - self.alpha[i])  \n                residuals.append(x * self.alpha[i])\n    \n        for i, dec in enumerate(self.decoders):\n            x = dec(x)\n            if i < self.n - 1:\n                x = x + residuals.pop()\n    \n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:35:13.009216Z","iopub.execute_input":"2025-05-12T16:35:13.009491Z","iopub.status.idle":"2025-05-12T16:35:13.015732Z","shell.execute_reply.started":"2025-05-12T16:35:13.009471Z","shell.execute_reply":"2025-05-12T16:35:13.014940Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"class CRNN(nn.Module):\n    def __init__(self, in_channels, kernel_size, pool_kernel_size, dropout, latent_dim, lstm_hidden_dim, vocab_size, output_length=5):\n        super().__init__()\n        self.lstm_hidden_dim = lstm_hidden_dim\n        self.output_length = output_length\n        self.vocab_size = vocab_size\n        \n        self.conv1 = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels, out_channels=in_channels*2, kernel_size=kernel_size, padding=2),\n            nn.BatchNorm2d(num_features=in_channels*2),\n            nn.ReLU(inplace=False),\n            nn.MaxPool2d(kernel_size=pool_kernel_size)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels*2, out_channels=in_channels*4, kernel_size=kernel_size, padding=2),\n            nn.BatchNorm2d(num_features=in_channels*4),\n            nn.ReLU(inplace=False),\n            nn.MaxPool2d(kernel_size=pool_kernel_size)\n        )\n        self.flatten = nn.Flatten()\n        self.dropout = nn.Dropout(dropout)\n        self.latent_fc = nn.LazyLinear(latent_dim)\n        self.lstm = nn.LSTM(input_size=latent_dim, hidden_size=lstm_hidden_dim, num_layers=1, batch_first=True)\n        self.output_fc = nn.Linear(lstm_hidden_dim, vocab_size)\n    \n    def forward(self, x):\n        batch_size = x.size(0)\n        \n        conv1_out = self.conv1(x)\n        conv2_out = self.conv2(conv1_out)\n        flattened = self.flatten(conv2_out)\n        dropped = self.dropout(flattened)\n        latent = self.latent_fc(dropped)\n        \n        lstm_input = latent.unsqueeze(1)  # Shape: (batch_size, 1, latent_dim)\n        \n        h0 = torch.zeros(1, batch_size, self.lstm_hidden_dim, device=x.device)\n        c0 = torch.zeros(1, batch_size, self.lstm_hidden_dim, device=x.device)\n        \n        outputs = []\n        \n        for _ in range(self.output_length):\n            out, (h0, c0) = self.lstm(lstm_input, (h0, c0))  # out shape: (batch_size, 1, lstm_hidden_dim)\n            logits = self.output_fc(out.squeeze(1))  # Shape: (batch_size, vocab_size)\n            outputs.append(logits)\n            \n        outputs = torch.stack(outputs, dim=1)  # Shape: (batch_size, 5, vocab_size)\n        \n        return outputs\n        \n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:35:16.394152Z","iopub.execute_input":"2025-05-12T16:35:16.394413Z","iopub.status.idle":"2025-05-12T16:35:16.402363Z","shell.execute_reply.started":"2025-05-12T16:35:16.394394Z","shell.execute_reply":"2025-05-12T16:35:16.401537Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class CaptchaCrackNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.affn=AFFN(AFFN_DEPTH).to(device)\n\n        self.conv1=nn.Sequential(\n            nn.Conv2d(in_channels=1,out_channels=32,kernel_size=5,padding=2),\n            nn.ReLU(inplace=False),\n            nn.MaxPool2d(kernel_size=2)\n        )\n\n        self.conv2=nn.Sequential(\n                    nn.Conv2d(in_channels=32,out_channels=48,kernel_size=5,padding=2),\n                    nn.ReLU(inplace=False),\n                    nn.MaxPool2d(kernel_size=2)\n                )\n\n        self.conv3=nn.Sequential(\n            nn.Conv2d(in_channels=48,out_channels=64,kernel_size=5,padding=2),\n            nn.ReLU(inplace=False),\n            nn.MaxPool2d(kernel_size=2)\n        )\n\n        self.res=nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=2, padding=2)\n\n        self.crnn=CRNN(64,CRNN_KERNEL,CRNN_POOL_KERNEL,CRNN_DROPOUT,CRNN_LATENT,LSTM_HIDDEN_DIM,VOCAB_SIZE,OUTPUT_LENGTH).to(device)\n\n    def forward(self,x):\n        affn_out=self.affn(x)\n        res_out=self.res(x)\n        conv1_out=self.conv1(affn_out)\n        conv2_out=self.conv2(conv1_out+res_out)\n        conv3_out=self.conv3(conv2_out)\n        output=self.crnn(conv3_out)\n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:35:20.197538Z","iopub.execute_input":"2025-05-12T16:35:20.198238Z","iopub.status.idle":"2025-05-12T16:35:20.204462Z","shell.execute_reply.started":"2025-05-12T16:35:20.198213Z","shell.execute_reply":"2025-05-12T16:35:20.203702Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def loss_fn(preds,target):\n    ce_loss=F.cross_entropy(preds,target)\n    \n    return ce_loss\n\ndef asr(preds,target):\n    preds_argmax=torch.argmax(preds,dim=-1)\n    asr=(preds_argmax==target).sum(dim=-1)/preds.shape[1]\n    return asr.mean()\n\n\ndef train(model,train_loader,val_loader,optimizer,loss_fn,epochs):\n    train_history=[]\n    val_history=[]\n    asr_history=[]\n    model.to(device)\n    for epoch in range(1,epochs+1):\n        print(f\"Epoch {epoch}:\")\n        model.train()\n        avg_loss=0\n        for batch_num,(X,y) in enumerate(tqdm(train_loader,desc=\"Progress: \")):\n            X=X.to(device)\n            y=y.to(device)\n            optimizer.zero_grad()\n            preds=model(X)\n            \n            loss=loss_fn(preds.view(-1, VOCAB_SIZE),y.view(-1))\n            loss.backward()\n            optimizer.step()\n            \n            avg_loss+=loss.item()\n        avg_loss/=len(train_loader)\n        train_history.append(avg_loss)\n        print(f\"Loss: {avg_loss}\")\n\n        eval_loss=0\n        asr_avg=0\n        if VAL_EPOCH and epoch%VAL_EPOCH==0:\n            model.eval()\n            with torch.no_grad():\n                for batch_num,(X,y) in enumerate(tqdm(val_loader,desc=\"Progress: \")):\n                    X=X.to(device)\n                    y=y.to(device)\n                    preds=model(X)\n                    loss=loss_fn(preds.view(-1, VOCAB_SIZE),y.view(-1))\n        \n                    eval_loss+=loss.item()\n                    \n                    asr_val=asr(preds,y)\n                    asr_avg+=asr_val.item()\n                    \n                eval_loss/=len(val_loader)\n                asr_avg/=len(val_loader)\n                asr_history.append(asr_avg)\n                val_history.append(eval_loss)\n                print(f\"Val Loss: {eval_loss}\",end=' ')\n                print(f\"Val ASR: {asr_avg}\")\n\n        if SAVE_EPOCH and epoch%SAVE_EPOCH==0:\n            print(\"Saving model\")\n            path=str(epoch)+'.pth'\n            torch.save(model.state_dict(), path)\n    torch.save(model.state_dict(),'final.pth')\n    return train_history,val_history,asr_history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:35:23.891162Z","iopub.execute_input":"2025-05-12T16:35:23.891905Z","iopub.status.idle":"2025-05-12T16:35:23.903081Z","shell.execute_reply.started":"2025-05-12T16:35:23.891879Z","shell.execute_reply":"2025-05-12T16:35:23.902438Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"model=CaptchaCrackNet().to(device)\noptimizer=torch.optim.Adam(model.parameters())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-12T16:35:27.548920Z","iopub.execute_input":"2025-05-12T16:35:27.549177Z","iopub.status.idle":"2025-05-12T16:35:27.778300Z","shell.execute_reply.started":"2025-05-12T16:35:27.549158Z","shell.execute_reply":"2025-05-12T16:35:27.777757Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"train_history,val_history,=train(model,train_loader,val_loader,optimizer,nn.CrossEntropyLoss(),EPOCHS)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(train_history,label='train')\nplt.plot(val_history,label='val')\nplt.title(\"Loss\")\nplt.legend()\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-13T03:31:44.554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(asr_history)\nplt.title(\"Validation ASR\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"characters = string.ascii_letters + string.digits  # Uppercase + lowercase + digits\nidx_to_char = {idx: char for idx, char in enumerate(characters)}\ndef to_text(arr):\n    ans=''\n    for c in arr:\n        ans=ans+idx_to_char[c.item()]\n    return ans","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-13T03:31:44.554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nwith torch.no_grad():\n    for X,y in train_loader:\n        X=X[0]\n        \n        plt.imshow(X.numpy().transpose(1,2,0))\n        plt.show()\n        output=model(X.unsqueeze(0).to(device))\n        X1=model.affn(X.unsqueeze(0).to(device))\n        plt.imshow(X1[0].cpu().numpy().transpose(1,2,0))\n        plt.show()\n        print(output.shape)\n        print(to_text(output.squeeze(0).argmax(axis=1)))\n        break","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-13T03:31:44.554Z"}},"outputs":[],"execution_count":null}]}