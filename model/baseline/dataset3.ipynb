{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x4q3DsFQfmED"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "X7KLUhaSIZBr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "#from torchsummary import summary\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader,random_split,Dataset\n",
        "from PIL import Image,UnidentifiedImageError\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "geXGfiwDfxuP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.10), please consider upgrading to the latest version (0.3.12).\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"akashguna/large-captcha-dataset\")\n",
        "#Hyper parameters\n",
        "#DATA_DIR='/kaggle/input/captcha-dataset'\n",
        "DATA_DIR = '/kaggle/input/large-captcha-dataset/Large_Captcha_Dataset'\n",
        "BATCH_SIZE=128\n",
        "VAL_SPLIT=0.05\n",
        "\n",
        "#AFFN\n",
        "AFFN_KERNEL=5\n",
        "AFFN_STRIDE=1\n",
        "AFFN_DEPTH=1\n",
        "\n",
        "#CRNN\n",
        "CRNN_KERNEL=5\n",
        "CRNN_POOL_KERNEL=2\n",
        "CRNN_DROPOUT=0.3\n",
        "CRNN_LATENT=128\n",
        "LSTM_HIDDEN_DIM=32\n",
        "VOCAB_SIZE=26*2+10\n",
        "OUTPUT_LENGTH=5\n",
        "\n",
        "#Train\n",
        "SAVE_EPOCH=5\n",
        "VAL_EPOCH=1\n",
        "EPOCHS=20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXgZGvLNdy8A",
        "outputId": "a9a05753-d0fc-4439-a019-2f4e0bc2a816"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/kaggle/input/large-captcha-dataset\n"
          ]
        }
      ],
      "source": [
        "print(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT5Ug2NOxqQq",
        "outputId": "0bc8765b-e0d6-4675-8e33-55868fd55950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error occurred while trying to remove the image: [Errno 30] Read-only file system: '/kaggle/input/large-captcha-dataset/Large_Captcha_Dataset/4q2wA.png'\n"
          ]
        }
      ],
      "source": [
        "def remove_image(image_path):\n",
        "    try:\n",
        "        if os.path.exists(image_path):\n",
        "            os.remove(image_path)\n",
        "            print(f\"Successfully removed the image: {image_path}\")\n",
        "        else:\n",
        "            print(f\"Image not found: {image_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred while trying to remove the image: {e}\")\n",
        "\n",
        "\n",
        "image_to_remove = f'{DATA_DIR}/4q2wA.png'\n",
        "remove_image(image_to_remove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vHUxpaiq3JPV"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = '/kaggle/input/large-captcha-dataset/Large_Captcha_Dataset'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9zJh4yfgrHa",
        "outputId": "ec81aaad-957f-437f-95cf-03f24b032ae4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device  = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gPG--Vpi5ufa"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "chars = string.ascii_letters + string.digits\n",
        "char_idx_dict = {character: index for index, character in enumerate(chars)}\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        self.labels_indices = []\n",
        "\n",
        "        for f in os.listdir(root_dir):\n",
        "            if f.lower().endswith(('png', 'jpg', 'jpeg')):\n",
        "                img_path = os.path.join(root_dir, f)\n",
        "                try:\n",
        "                    with Image.open(img_path) as img:\n",
        "                        img.verify()\n",
        "                    self.image_paths.append(img_path)\n",
        "                    label_str = os.path.splitext(f)[0]\n",
        "                    self.labels.append(label_str)\n",
        "                    label_idx = [char_idx_dict[c] for c in label_str if c in char_idx_dict]\n",
        "                    self.labels_indices.append(torch.tensor(label_idx, dtype=torch.long))\n",
        "                except (UnidentifiedImageError, IOError) as e:\n",
        "                    logging.warning(f\"Skipping invalid image file {img_path}. Error: {e}\")\n",
        "\n",
        "        logging.info(f\"Loaded {len(self.image_paths)} valid images from {root_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Defensive: try to find a valid image up to dataset size attempts to avoid infinite loops\n",
        "        attempts = 0\n",
        "        size = len(self.image_paths)\n",
        "        while attempts < size:\n",
        "            img_path = self.image_paths[index]\n",
        "            try:\n",
        "                with Image.open(img_path) as image:\n",
        "                    image = image.convert('RGB')\n",
        "                label_tensor = self.labels_indices[index]\n",
        "                if self.transform:\n",
        "                    image = self.transform(image)\n",
        "                return image, label_tensor\n",
        "            except (UnidentifiedImageError, IOError) as e:\n",
        "                logging.warning(f\"Skipping problematic image file {img_path}. Error: {e}\")\n",
        "                index = (index + 1) % size\n",
        "                attempts += 1\n",
        "\n",
        "        raise RuntimeError(\"No valid images found in the dataset.\")\n",
        "\n",
        "def get_dataloader(data_directory, batch_size=32, val_split=0.2, shuffle=True, num_workers=0):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((40, 150)),\n",
        "        transforms.Grayscale(num_output_channels=1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda x: x / 255.0)\n",
        "    ])\n",
        "\n",
        "    dataset = CustomDataset(root_dir=data_directory, transform=transform)\n",
        "    total_size = len(dataset)\n",
        "    if total_size == 0:\n",
        "        raise RuntimeError(\"No valid images found in the dataset directory.\")\n",
        "\n",
        "    val_size = int(total_size * val_split)\n",
        "    train_size = total_size - val_size\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyCDaVQNun5v",
        "outputId": "12b419f3-e084-4aa8-834b-3603d09263bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Skipping invalid image file /kaggle/input/large-captcha-dataset/Large_Captcha_Dataset/4q2wA.png. Error: cannot identify image file '/kaggle/input/large-captcha-dataset/Large_Captcha_Dataset/4q2wA.png'\n"
          ]
        }
      ],
      "source": [
        "train_loader, val_loader = get_dataloader(data_directory=DATA_DIR,batch_size=BATCH_SIZE,val_split=VAL_SPLIT,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMCBCdwlx2Ox",
        "outputId": "f627613e-3695-48da-fc6e-2a7a90cfdeda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 1, 40, 150])\n",
            "torch.Size([128, 5])\n"
          ]
        }
      ],
      "source": [
        "X,y = next(iter(train_loader))\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZIMb39M80M3P"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Sequential):\n",
        "  def __init__(self,n,kernel_size,stride):\n",
        "    super().__init__(\n",
        "        nn.Conv2d(in_channels=4**(n-1),out_channels=4**n,kernel_size=kernel_size,stride=stride),\n",
        "        nn.BatchNorm2d(num_features=4**n),\n",
        "        nn.ReLU(inplace=False)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4NYvGT7vnV2y"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Sequential):\n",
        "  def __init__(self,n,kernel_size,stride):\n",
        "    super().__init__(\n",
        "        nn.ConvTranspose2d(in_channels=4**n,out_channels=4**(n-1),kernel_size=kernel_size,stride=stride),\n",
        "        nn.BatchNorm2d(num_features=4**(n-1)),\n",
        "        nn.ReLU(inplace=False)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "3REByABFq7CH"
      },
      "outputs": [],
      "source": [
        "class AFFN(nn.Module):\n",
        "  def __init__(self,n):\n",
        "    super().__init__()\n",
        "    self.n= n\n",
        "    self.alpha = nn.Parameter(torch.randn(n-1).to(device)).to(device)\n",
        "    self.encoders = []\n",
        "    self.decoders = []\n",
        "    for  i in range(1,n+1):\n",
        "      self.encoders.append(Encoder(i,AFFN_KERNEL,AFFN_STRIDE).to(device))\n",
        "    for  i in range(n,0,-1):\n",
        "      self.decoders.append(Decoder(i,AFFN_KERNEL,AFFN_STRIDE).to(device))\n",
        "\n",
        "  def forward(self,x):\n",
        "    residuals = []\n",
        "    for i,enc in enumerate(self.encoders):\n",
        "      x= enc(x)\n",
        "      if i < self.n-1:\n",
        "        x = x * (1 - self.alpha[i])\n",
        "        residuals.append(x * self.alpha[i])\n",
        "    for i,dec in enumerate(self.decoders):\n",
        "      x= dec(x)\n",
        "      if i < self.n-1:\n",
        "        x= x + residuals.pop()\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BHct8LzoucU3"
      },
      "outputs": [],
      "source": [
        "class CRNN(nn.Module):\n",
        "    def __init__(self, in_channels, kernel_size, pool_kernel_size, dropout, latent_dim, lstm_hidden_dim, vocab_size, output_length=5):\n",
        "        super().__init__()\n",
        "        self.lstm_hidden_dim = lstm_hidden_dim\n",
        "        self.output_length = output_length  # Should be 5 for 5 characters\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels, out_channels=in_channels*2, kernel_size=kernel_size, padding=2),\n",
        "            nn.BatchNorm2d(num_features=in_channels*2),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.MaxPool2d(kernel_size=pool_kernel_size)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels*2, out_channels=in_channels*4, kernel_size=kernel_size, padding=2),\n",
        "            nn.BatchNorm2d(num_features=in_channels*4),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.MaxPool2d(kernel_size=pool_kernel_size)\n",
        "        )\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.latent_fc = nn.LazyLinear(latent_dim)\n",
        "        self.lstm = nn.LSTM(input_size=latent_dim, hidden_size=lstm_hidden_dim, num_layers=1, batch_first=True)\n",
        "        self.output_fc = nn.Linear(lstm_hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # CNN feature extraction\n",
        "        conv1_out = self.conv1(x)\n",
        "        conv2_out = self.conv2(conv1_out)\n",
        "        flattened = self.flatten(conv2_out)\n",
        "        dropped = self.dropout(flattened)\n",
        "        latent = self.latent_fc(dropped)\n",
        "\n",
        "        lstm_input = latent.unsqueeze(1)\n",
        "\n",
        "        # Initialize hidden and cell states\n",
        "        h0 = torch.zeros(1, batch_size, self.lstm_hidden_dim, device=x.device)\n",
        "        c0 = torch.zeros(1, batch_size, self.lstm_hidden_dim, device=x.device)\n",
        "\n",
        "        outputs = []\n",
        "\n",
        "        # Generate 5 characters sequentially\n",
        "        for _ in range(self.output_length):\n",
        "            out, (h0, c0) = self.lstm(lstm_input, (h0, c0))  # out shape: (batch_size, 1, lstm_hidden_dim)\n",
        "\n",
        "            logits = self.output_fc(out.squeeze(1))  # Shape: (batch_size, vocab_size)\n",
        "\n",
        "            outputs.append(logits)\n",
        "\n",
        "        outputs = torch.stack(outputs, dim=1)  # Shape: (batch_size, 5, vocab_size)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kSBbC0g63YK7"
      },
      "outputs": [],
      "source": [
        "output=CRNN(64,CRNN_KERNEL,CRNN_POOL_KERNEL,CRNN_DROPOUT,CRNN_LATENT,LSTM_HIDDEN_DIM,VOCAB_SIZE,OUTPUT_LENGTH).to(device)(torch.zeros((2,64,256,256)).to(device))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yljn6Qos3mT6",
        "outputId": "b2ebe336-b456-49df-a109-cb92f2fdf0df"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 5, 62])"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vrl3Dh_a3tdN"
      },
      "outputs": [],
      "source": [
        "class CaptchaCrackNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.affn=AFFN(AFFN_DEPTH).to(device)\n",
        "\n",
        "        self.conv1=nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1,out_channels=32,kernel_size=5,padding=2),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        self.conv2=nn.Sequential(\n",
        "                    nn.Conv2d(in_channels=32,out_channels=48,kernel_size=5,padding=2),\n",
        "                    nn.ReLU(inplace=False),\n",
        "                    nn.MaxPool2d(kernel_size=2)\n",
        "                )\n",
        "\n",
        "        self.conv3=nn.Sequential(\n",
        "            nn.Conv2d(in_channels=48,out_channels=64,kernel_size=5,padding=2),\n",
        "            nn.ReLU(inplace=False),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "\n",
        "        self.res=nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=2, padding=2)\n",
        "\n",
        "        self.crnn=CRNN(64,CRNN_KERNEL,CRNN_POOL_KERNEL,CRNN_DROPOUT,CRNN_LATENT,LSTM_HIDDEN_DIM,VOCAB_SIZE,OUTPUT_LENGTH).to(device)\n",
        "\n",
        "    def forward(self,x):\n",
        "        affn_out=self.affn(x)\n",
        "        res_out=self.res(x)\n",
        "        conv1_out=self.conv1(affn_out)\n",
        "        conv2_out=self.conv2(conv1_out+res_out)\n",
        "        conv3_out=self.conv3(conv2_out)\n",
        "        output=self.crnn(conv3_out)\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUuh1k2u37n1",
        "outputId": "057b1284-45e1-4478-d6d3-88bf74820df0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CaptchaCrackNet(\n",
              "  (affn): AFFN()\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv2): Sequential(\n",
              "    (0): Conv2d(32, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv3): Sequential(\n",
              "    (0): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (res): Conv2d(1, 32, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
              "  (crnn): CRNN(\n",
              "    (conv1): Sequential(\n",
              "      (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (conv2): Sequential(\n",
              "      (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
              "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "    (dropout): Dropout(p=0.3, inplace=False)\n",
              "    (latent_fc): LazyLinear(in_features=0, out_features=128, bias=True)\n",
              "    (lstm): LSTM(128, 32, batch_first=True)\n",
              "    (output_fc): Linear(in_features=32, out_features=62, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "CaptchaCrackNet()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "K5HKuCmY4AVc"
      },
      "outputs": [],
      "source": [
        "def loss_fn(preds,target):\n",
        "    ce_loss=F.cross_entropy(preds,target)\n",
        "    return ce_loss\n",
        "\n",
        "torch.manual_seed(42)\n",
        "def train(model,train_loader,val_loader,optimizer,loss_fn,epochs):\n",
        "    train_history=[]\n",
        "    val_history=[]\n",
        "    model.to(device)\n",
        "    for epoch in range(1,epochs+1):\n",
        "        print(f\"Epoch {epoch}:\")\n",
        "        model.train()\n",
        "        avg_loss=0\n",
        "        for batch_num,(X,y) in enumerate(tqdm(train_loader,desc=\"Progress: \")):\n",
        "            X=X.to(device)\n",
        "            y=y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            preds=model(X)\n",
        "\n",
        "            loss=loss_fn(preds.reshape(-1, VOCAB_SIZE),y.view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            avg_loss+=loss.item()\n",
        "        avg_loss/=len(train_loader)\n",
        "        train_history.append(avg_loss)\n",
        "        print(f\"Loss: {avg_loss}\")\n",
        "\n",
        "        eval_loss=0\n",
        "        if VAL_EPOCH and epoch%VAL_EPOCH==0:\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                for batch_num,(X,y) in enumerate(tqdm(val_loader,desc=\"Progress: \")):\n",
        "                    X=X.to(device)\n",
        "                    y=y.to(device)\n",
        "                    preds=model(X)\n",
        "                    loss = loss_fn(preds.reshape(-1, VOCAB_SIZE), y.reshape(-1))\n",
        "\n",
        "                    eval_loss+=loss.item()\n",
        "                eval_loss/=len(val_loader)\n",
        "                val_history.append(eval_loss)\n",
        "                print(f\"Val Loss: {eval_loss}\",eval_loss)\n",
        "\n",
        "        if SAVE_EPOCH and epoch%SAVE_EPOCH==0:\n",
        "            print(\"Saving model\")\n",
        "            path=str(epoch)+'.pth'\n",
        "            torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'seed': 42\n",
        "}, path)\n",
        "    torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'seed': 42\n",
        "}, 'final final.pth')\n",
        "    return train_history,val_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JdPi0K0hUUbH"
      },
      "outputs": [],
      "source": [
        "\n",
        "model=CaptchaCrackNet().to(device)\n",
        "optimizer=torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yj7ifu_WxRjs",
        "outputId": "36e30209-5cd7-4750-b315-66c99c63891a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint keys: dict_keys(['model_state_dict', 'optimizer_state_dict', 'seed'])\n"
          ]
        }
      ],
      "source": [
        "checkpoint = torch.load(r'/content/10 add gc.pth', map_location=device)\n",
        "# Restore states\n",
        "print(\"Checkpoint keys:\", checkpoint.keys())\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujBvJBGmV-We"
      },
      "outputs": [],
      "source": [
        "#train_history,val_history=train(model,train_loader,val_loader,optimizer,nn.CrossEntropyLoss(),EPOCHS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wo2qA4LTXLO5"
      },
      "outputs": [],
      "source": [
        "# x_values = list(range(1, EPOCHS + 1))\n",
        "# plt.plot(x_values, train_history, label='Train')\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('Loss')\n",
        "# plt.legend()\n",
        "# plt.title('Training History')\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "oDZJHWdUvtZQ"
      },
      "outputs": [],
      "source": [
        "characters = string.ascii_letters + string.digits\n",
        "idx_to_char = {idx: char for idx, char in enumerate(characters)}\n",
        "def to_text(arr):\n",
        "    ans=''\n",
        "    for c in arr:\n",
        "        ans=ans+idx_to_char[c.item()]\n",
        "    return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "noCKdLvA39Cy",
        "outputId": "d5ea82ff-0788-44ff-8656-74b96c0b728d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output shape: torch.Size([1, 5, 62])\n",
            "Predicted indices: [60, 52, 52, 29, 57]\n",
            "Predicted text: 800D5\n",
            "Ground truth label: tensor([60, 52, 52, 29, 57])\n",
            "Actual label: 800D5\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    for X, y in train_loader:\n",
        "        X_sample = X[0]\n",
        "        y_sample = y[0]\n",
        "\n",
        "        img = X_sample.cpu().numpy().transpose(1, 2, 0)\n",
        "        plt.imshow(img)\n",
        "        plt.title(\"Input Image\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        X_input = X_sample.unsqueeze(0).to(device)\n",
        "        output = model(X_input)\n",
        "        X1 = model.affn(X_input)\n",
        "\n",
        "        img_trans = X1[0].cpu().numpy().transpose(1, 2, 0)\n",
        "        plt.imshow(img_trans)\n",
        "        plt.title(\"Transformed Image\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "        print(\"Output shape:\", output.shape)\n",
        "\n",
        "        pred_indices = output.squeeze(0).argmax(dim=1)\n",
        "        print(\"Predicted indices:\", pred_indices.tolist())\n",
        "        print(\"Predicted text:\", to_text(pred_indices))\n",
        "        print(\"Ground truth label:\", y_sample)\n",
        "        print(\"Actual label:\",to_text(y_sample))\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "6cZE9R8B98dz"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    total_samples = 0\n",
        "    correct_samples = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "\n",
        "            output = output.reshape(-1, output.shape[-1])\n",
        "            target = target.view(-1)\n",
        "\n",
        "            loss = criterion(output, target)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "            correct_samples += pred.eq(target).sum().item()\n",
        "            total_samples += target.numel()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = 100.0 * correct_samples / total_samples\n",
        "    print(f'Average test loss: {avg_loss:.4f}, Accuracy: {correct_samples}/{total_samples} ({accuracy:.2f}%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaeUeXGb9PmY",
        "outputId": "dafe6ecb-24e4-403f-fc8f-e0dd4169c491"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average test loss: 0.0312, Accuracy: 20429/20580 (99.27%)\n"
          ]
        }
      ],
      "source": [
        "evaluate(model,data_loader=val_loader,criterion=nn.CrossEntropyLoss())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (pytorch_env)",
      "language": "python",
      "name": "pytorch_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
